{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(\"Previous reviewer Claudio Carvalho gave a much better recap of the film's plot details than I could. What I recall mostly is that it was just so beautiful, in every sense - emotionally, visually, editorially - just gorgeous.<br /><br />If you like movies that are wonderful to look at, and also have emotional content to which that beauty is relevant, I think you will be glad to have seen this extraordinary and unusual work of art.<br /><br />On a scale of 1 to 10, I'd give it about an 8.75. The only reason I shy away from 9 is that it is a mood piece. If you are in the mood for a really artistic, very romantic film, then it's a 10. I definitely think it's a must-see, but none of us can be in that mood all the time, so, overall, 8.75.\",\n 1)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_SAMPLES = 500\n",
    "\n",
    "with open(f\"sentiment-dataset-{N_SAMPLES}.pickle\", \"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "train_x, train_y, test_x, test_y = dataset\n",
    "\n",
    "train_x[0], train_y[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "451"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_compressed = len(gzip.compress(train_x[0].encode(\"utf-8\")))\n",
    "x_compressed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "639"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2_compressed = len(gzip.compress(train_x[1].encode(\"utf-8\")))\n",
    "x2_compressed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "1024"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx2 = len(gzip.compress((\" \".join([train_x[0], train_x[1]])).encode(\"utf-8\")))\n",
    "xx2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8967136150234741"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncd = (xx2 - min(x_compressed, x2_compressed)) / max(x_compressed, x2_compressed)\n",
    "ncd"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def ncd(x, x2):\n",
    "    x_compressed = len(gzip.compress(x.encode(\"utf-8\")))\n",
    "    x2_compressed = len(gzip.compress(x2.encode(\"utf-8\")))\n",
    "    xx2 = len(gzip.compress((\" \".join([x, x2])).encode(\"utf-8\")))\n",
    "    return (xx2 - min(x_compressed, x2_compressed)) / max(x_compressed, x2_compressed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "train_ncd = [[ncd(train_x[i], train_x[j]) for j in range(len(train_x))] for i in range(len(train_x))]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.026607538802660754,\n 0.8967136150234741,\n 0.8337028824833703,\n 0.844789356984479,\n 0.83991683991684,\n 0.8226164079822617,\n 0.8969072164948454,\n 0.8021505376344086,\n 0.8631790744466801,\n 0.8625277161862528,\n 0.8048780487804879,\n 0.8824833702882483,\n 0.862992125984252,\n 0.8520408163265306,\n 0.8512544802867383,\n 0.835920177383592,\n 0.8603104212860311,\n 0.811529933481153,\n 0.8580931263858093,\n 0.8755980861244019,\n 0.8181818181818182,\n 0.8314855875831486,\n 0.9324866310160428,\n 0.8489361702127659,\n 0.9553372041089773,\n 0.8717105263157895,\n 0.8492239467849224,\n 0.9112627986348123,\n 0.8381374722838137,\n 0.8470066518847007,\n 0.8350515463917526,\n 0.8425720620842572,\n 0.8871866295264624,\n 0.9157581764122894,\n 0.835920177383592,\n 0.8898963730569949,\n 0.9588457899716177,\n 0.8514412416851441,\n 0.8217391304347826,\n 0.85,\n 0.8582089552238806,\n 0.8536585365853658,\n 0.835920177383592,\n 0.8615384615384616,\n 0.8292682926829268,\n 0.9052734375,\n 0.8625277161862528,\n 0.8812392426850258,\n 0.9430455635491607,\n 0.8381374722838137,\n 0.9491358024691358,\n 0.844789356984479,\n 0.925849639546859,\n 0.8514412416851441,\n 0.8314855875831486,\n 0.8098290598290598,\n 0.9269406392694064,\n 0.8326086956521739,\n 0.9592743428359867,\n 0.8480492813141683,\n 0.9333826794966691,\n 0.9278969957081545,\n 0.8691796008869179,\n 0.8669623059866962,\n 0.8425720620842572,\n 0.8741258741258742,\n 0.8580562659846548,\n 0.9195046439628483,\n 0.8536585365853658,\n 0.8203991130820399,\n 0.9201277955271565,\n 0.8373015873015873,\n 0.8767361111111112,\n 0.8823529411764706,\n 0.8270509977827051,\n 0.8663446054750402,\n 0.8839009287925697,\n 0.8392857142857143,\n 0.8295019157088123,\n 0.8463035019455253,\n 0.8492239467849224,\n 0.8609865470852018,\n 0.9063636363636364,\n 0.8189845474613686,\n 0.8226164079822617,\n 0.8773747841105354,\n 0.9227974568574023,\n 0.9525888958203369,\n 0.82560706401766,\n 0.9188224799286352,\n 0.835920177383592,\n 0.865625,\n 0.8691796008869179,\n 0.8382899628252788,\n 0.9049826187717266,\n 0.9534883720930233,\n 0.8824306472919419,\n 0.8661870503597122,\n 0.8657487091222031,\n 0.835920177383592,\n 0.9261477045908184,\n 0.9015837104072398,\n 0.8558758314855875,\n 0.8613518197573656,\n 0.8548707753479126,\n 0.9030303030303031,\n 0.8558758314855875,\n 0.873758865248227,\n 0.9336206896551724,\n 0.8586065573770492,\n 0.8248337028824834,\n 0.8514412416851441,\n 0.8330134357005758,\n 0.8230088495575221,\n 0.8203991130820399,\n 0.8473581213307241,\n 0.8248337028824834,\n 0.9344709897610921,\n 0.8863976083707026,\n 0.9656322989656323,\n 0.8290598290598291,\n 0.8764044943820225,\n 0.8870056497175142,\n 0.9311497326203209,\n 0.9442622950819672,\n 0.87518573551263,\n 0.950965824665676,\n 0.8440860215053764,\n 0.8935309973045822,\n 0.900871459694989,\n 0.8625277161862528,\n 0.9569183983781044,\n 0.8625277161862528,\n 0.887719298245614,\n 0.907537688442211,\n 0.9313087490961678,\n 0.9053892215568863,\n 0.8852459016393442,\n 0.9087837837837838,\n 0.8558758314855875,\n 0.8608695652173913,\n 0.8774509803921569,\n 0.9186923721709975,\n 0.8929016189290162,\n 0.8579654510556622,\n 0.8398268398268398,\n 0.8691796008869179,\n 0.8381374722838137,\n 0.8654781199351702,\n 0.8456659619450317,\n 0.9233226837060703,\n 0.838495575221239,\n 0.8679577464788732,\n 0.9036402569593148,\n 0.9085872576177285,\n 0.8492239467849224,\n 0.8514412416851441,\n 0.8425720620842572,\n 0.8511796733212341,\n 0.9179170344218888,\n 0.8470066518847007,\n 0.8516949152542372,\n 0.835920177383592,\n 0.8226164079822617,\n 0.8859259259259259,\n 0.8580931263858093,\n 0.902229845626072,\n 0.8514412416851441,\n 0.8923076923076924,\n 0.8292682926829268,\n 0.9358288770053476,\n 0.8337028824833703,\n 0.835920177383592,\n 0.8603104212860311,\n 0.8536585365853658,\n 0.8647450110864745,\n 0.9079401611047181,\n 0.8159645232815964,\n 0.8580931263858093,\n 0.8314855875831486,\n 0.8822674418604651,\n 0.9510143493320139,\n 0.9019607843137255,\n 0.8470066518847007,\n 0.9542168674698795,\n 0.8858800773694391,\n 0.8492239467849224,\n 0.8603104212860311,\n 0.8625277161862528,\n 0.9122055674518201,\n 0.83991683991684,\n 0.8732612055641422,\n 0.851063829787234,\n 0.8722689075630252,\n 0.8833592534992224,\n 0.8770491803278688,\n 0.9169295478443743,\n 0.8558758314855875,\n 0.8416666666666667,\n 0.8558758314855875,\n 0.9119420989143546,\n 0.8774002954209749,\n 0.8521739130434782,\n 0.8691796008869179,\n 0.9242685025817556,\n 0.8580931263858093,\n 0.835920177383592,\n 0.9309859154929577,\n 0.8957871396895787,\n 0.8203991130820399,\n 0.8226164079822617,\n 0.9504132231404959,\n 0.9235033259423503,\n 0.859304084720121,\n 0.8580931263858093,\n 0.8688524590163934,\n 0.9293193717277487,\n 0.9276054097056484,\n 0.8709016393442623,\n 0.90625,\n 0.8625277161862528,\n 0.8625277161862528,\n 0.8594594594594595,\n 0.8516377649325626,\n 0.8292682926829268,\n 0.9249812453113279,\n 0.8940149625935162,\n 0.9078624078624079,\n 0.8492239467849224,\n 0.8381374722838137,\n 0.8903061224489796,\n 0.872,\n 0.8314855875831486,\n 0.8536585365853658,\n 0.897982062780269,\n 0.8492239467849224,\n 0.93503800967519,\n 0.9167717528373266,\n 0.8603104212860311,\n 0.9060324825986079,\n 0.8270509977827051,\n 0.811529933481153,\n 0.8174097664543525,\n 0.9486500794070937,\n 0.9330007127583749,\n 0.8497576736672051,\n 0.8603104212860311,\n 0.8292682926829268,\n 0.9316298342541437,\n 0.8208955223880597,\n 0.9162833486660533,\n 0.8093126385809313,\n 0.8137472283813747,\n 0.9649202733485194,\n 0.8723994452149791,\n 0.8952496954933008,\n 0.8492239467849224,\n 0.8425720620842572,\n 0.9094827586206896,\n 0.8226164079822617,\n 0.8403547671840355,\n 0.8470066518847007,\n 0.8625277161862528,\n 0.8558758314855875,\n 0.8070953436807096,\n 0.8945409429280397,\n 0.8669623059866962,\n 0.844789356984479,\n 0.8713968957871396,\n 0.8779472954230236,\n 0.8701030927835052,\n 0.888,\n 0.8270509977827051,\n 0.8641571194762684,\n 0.8780487804878049,\n 0.8381374722838137,\n 0.8137472283813747,\n 0.9554384283660757,\n 0.8462897526501767,\n 0.9386026817219478,\n 0.9276504297994269,\n 0.8925869894099848,\n 0.8881987577639752,\n 0.8647450110864745,\n 0.8471953578336557,\n 0.8181818181818182,\n 0.9077380952380952,\n 0.8479087452471483,\n 0.8779411764705882,\n 0.9480375898286346,\n 0.8286334056399133,\n 0.8761904761904762,\n 0.8822553897180763,\n 0.8638743455497382,\n 0.8381374722838137,\n 0.808695652173913,\n 0.8814531548757171,\n 0.9527220630372493,\n 0.8461538461538461,\n 0.9340909090909091,\n 0.9097387173396675,\n 0.884080370942813,\n 0.8314855875831486,\n 0.8558758314855875,\n 0.8536585365853658,\n 0.9367854741089442,\n 0.8514412416851441,\n 0.9071644803229062,\n 0.9274809160305344,\n 0.8181818181818182,\n 0.8713968957871396,\n 0.9089301503094607,\n 0.8403547671840355,\n 0.8568738229755178,\n 0.8669623059866962,\n 0.8913525498891353,\n 0.8685446009389671,\n 0.8457350272232305,\n 0.8913525498891353,\n 0.8386454183266933,\n 0.9181395348837209,\n 0.8603104212860311,\n 0.8137472283813747,\n 0.9042253521126761,\n 0.8871428571428571,\n 0.835920177383592,\n 0.8492239467849224,\n 0.8385093167701864,\n 0.9526909722222222,\n 0.8736141906873615,\n 0.8492239467849224,\n 0.8470066518847007,\n 0.887987012987013,\n 0.932788374205268,\n 0.8736141906873615,\n 0.8713968957871396,\n 0.8603104212860311,\n 0.8713968957871396,\n 0.9215143120960295,\n 0.9401709401709402,\n 0.8532494758909853,\n 0.9051254089422028,\n 0.8492239467849224,\n 0.8470066518847007,\n 0.8873015873015873,\n 0.8203991130820399,\n 0.8226164079822617,\n 0.8924870466321243,\n 0.9461388708630759,\n 0.8793650793650793,\n 0.8270509977827051,\n 0.8701517706576728,\n 0.8433476394849786,\n 0.8625277161862528,\n 0.8625277161862528,\n 0.9138858988159311,\n 0.9227941176470589,\n 0.8580931263858093,\n 0.8514412416851441,\n 0.9047085201793722,\n 0.8048780487804879,\n 0.9245463228271251,\n 0.9199048374306106,\n 0.8470066518847007,\n 0.8402489626556017,\n 0.835920177383592,\n 0.861904761904762,\n 0.8857938718662952,\n 0.8403547671840355,\n 0.8556034482758621,\n 0.8558758314855875,\n 0.864406779661017,\n 0.8691796008869179,\n 0.8967828418230563,\n 0.8203991130820399,\n 0.8833333333333333,\n 0.8866396761133604,\n 0.8666666666666667,\n 0.8794835007173601,\n 0.844789356984479,\n 0.9269893355209188,\n 0.8892455858747994,\n 0.8492239467849224,\n 0.843010752688172,\n 0.8270509977827051,\n 0.9056795131845842,\n 0.8425720620842572,\n 0.9200376293508937,\n 0.8910256410256411,\n 0.8425720620842572,\n 0.8827470686767169,\n 0.881578947368421,\n 0.8349900596421471,\n 0.8619631901840491,\n 0.8558758314855875,\n 0.8937728937728938,\n 0.8956714761376249,\n 0.8492239467849224,\n 0.8337028824833703,\n 0.8708133971291866,\n 0.8470066518847007]"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_ncd[0]))\n",
    "train_ncd[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "test_ncd = [[ncd(test_x[i], train_x[j]) for j in range(len(train_x))] for i in range(len(test_x))]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "KNeighborsClassifier(n_neighbors=7)",
      "text/html": "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=7)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=7)</pre></div></div></div></div></div>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=7)\n",
    "neigh.fit(train_ncd, train_y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7029702970297029\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {neigh.score(test_ncd, test_y)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "70% accuracy with 500 samples and only a few lines of code. Not bad!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sped up version"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import multiprocessing\n",
    "\n",
    "NUM_PROCESSES = multiprocessing.cpu_count()\n",
    "N_SAMPLES = 10000\n",
    "\n",
    "with open(f\"sentiment-dataset-{N_SAMPLES}.pickle\", \"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "train_x, train_y, test_x, test_y = dataset\n",
    "\n",
    "def ncd(x, x2):\n",
    "    x_compressed = len(gzip.compress(x.encode(\"utf-8\")))\n",
    "    x2_compressed = len(gzip.compress(x2.encode(\"utf-8\")))\n",
    "    xx2 = len(gzip.compress((\" \".join([x, x2])).encode(\"utf-8\")))\n",
    "    return (xx2 - min(x_compressed, x2_compressed)) / max(x_compressed, x2_compressed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "# initialize NCD matricies\n",
    "train_ncd = [[0] * len(train_x) for _ in range(len(train_x))]\n",
    "test_ncd = [[0] * len(train_x) for _ in range(len(test_x))]\n",
    "\n",
    "def ncd_worker(data_row: tuple[int, str]):\n",
    "    i = data_row[0]\n",
    "    row = [ncd(data_row[1], train_x[j]) for j in range(len(train_x))]\n",
    "    return i, row"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "with multiprocessing.Pool(NUM_PROCESSES) as pool:\n",
    "    train_data = enumerate(train_x)\n",
    "    train_results = pool.map(ncd_worker, train_data)\n",
    "\n",
    "    test_data = enumerate(test_x)\n",
    "    test_results = pool.map(ncd_worker, test_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "for i, row in train_results:\n",
    "    train_ncd[i] = row\n",
    "    for i, row in test_results:\n",
    "        test_ncd[i] = row"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7571214392803598\n"
     ]
    }
   ],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=7)\n",
    "neigh.fit(train_ncd, train_y)\n",
    "accuracy = neigh.score(test_ncd, test_y)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The reason why this works is because when two strings are similar, gzip will be able to compress the two strings concatenated together more effectively than two strings that are dissimilar."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
